<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Vgent">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision-Language Model, Multi-modal LLM, Spatial-Temporal Understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Vgent</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-five-sixths">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</h1>

            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <a href="https://xiaoqian-shen.github.io/" style="color:#008AD7;font-weight:normal;">Xiaoqian Shen</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://wx-zhang.github.io/" style="color:#008AD7;font-weight:normal;">Wenxuan Zhang</a><sup style="color:#6fbf73;">1</sup>,</span>
              <a href="https://junchen14.github.io/" style="color:#008AD7;font-weight:normal;">Jun Chen</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#ed4b82;">2</sup>,</span>
              <a href="https://www.mohamed-elhoseiny.com/" style="color:#008AD7;font-weight:normal;">Mohamed Elhoseiny</a><sup style="color:#6fbf73;">1</sup></span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>King Abdullah University of Science and Technology</span>
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>Meta AI</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span style="color:#fd7343; font-family: Helvetica, sans-serif; font-weight: bold">NeurIPS 2025 Spotlight</span>
            </div>

            <br>

              <div class="content has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/xiaoqian-shen/Vgent" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                </span>

              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <img src="static/images/teaser.jpg" width="100%"/>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Understanding and reasoning over long videos pose significant challenges for large
video language models (LVLMs) due to the difficulty in processing intensive video
tokens beyond context window and retaining long-term sequential information.
Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in process-
ing long context for Large Language Models (LLMs); however, applying RAG to
long video faces challenges such as disrupted temporal dependencies and inclusion
of irrelevant information that can hinder accurate reasoning. To address these limi-
tations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented
generation framework to enhance LVLMs for long video understanding. Our
approach introduces two key innovations: (i) It represents videos by structured
graphs with semantic relationships across video clips preserved to improve re-
trieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate
the reasoning limitation of LVLMs, which leverages structured verification to
reduce retrieval noise and facilitate the explicit aggregation of relevant informa-
tion across clips, resulting in more accurate and context-aware responses. We
comprehensively evaluate our framework with various open-source LVLMs on
three long-video understanding benchmarks. Our approach yielded an overall
performance improvement of 3.0%∼5.4% over base models on MLVU, and
outperformed state-of-the-art video RAG methods by 8.6%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Vgent Pipeline</h2>
        <h2 class="content has-text-justified">
          Pipeline of Vgent, a novel framework for long-context video understanding in the proposed
graph-based retrieval-reasoning-augmented generation paradigm. It consists of four key stages: (1)
Offline video graph construction: Builds a video graph by extracting knowledge from
long videos. (2) Graph-based retrieval: Retrieves relevant clips based on keywords
extracted from the user query. (3) Structured reasoning: Refines clips using structured
queries and aggregates information. (4) Multimodal augmented generation: Combines
refined clips and reasoning results to generate the final response.
        </h2>
        <img src="static/images/main.png" width="100%"/>
      </div>
    </div>
  </div>
</section>



<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Video Understanding Results</h2>
        <div class="myrow">
          <img src="static/images/result1.png" width="70%"/>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">RAG methods comparison</h2>
        <div class="myrow">
          <img src="static/images/result2.png" width="70%"/>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Examples</h2>
        <div class="myrow">
          <img src="static/images/example1.png" width="80%"/>
        </div>
        <br>
        <div class="myrow">
          <img src="static/images/example2.png" width="80%"/>
        </div>

      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation</h2><pre><code>@inproceedings{shen2025vgent,
    title={Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding},
    author={Shen, Xiaoqian and Zhang, Wenxuan and Chen, Jun and Elhoseiny, Mohamed},
    journal={Advances in Neural Information Processing Systems},
    year={2025}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.

          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
